{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# College Football Simulation with new pairing system\n",
    "### By Rodrigo Vargas and Daniel Yedidia\n",
    "Project for PIC-16B Python with Applications II, UCLA Spring 2023 <br>\n",
    "Github: https://github.com/dyeds/PIC-16B-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure: (Tentative)\n",
    "\n",
    "1. Information & Credits: CollegeFootballData.com, https://www.reddit.com/r/CFB/comments/qq553i/what_would_college_football_look_like_under_a/ Post by u/dethwing_.\n",
    "2. Explanation of the project and how to use the notebook\n",
    "3. Data Acquisition and Preprocessing. Working with CFBD and BingMaps API's and storing locally on Database\n",
    "4. Model creation using Tensorflow. Creating various models and motives on why to use Betting Lines.\n",
    "5. Explanation and Implementation of Minimum Weight Matching Algorithm using Networkx for pairings and using distances.\n",
    "6. SQL Database with Simulated Games and other table creating Functions.\n",
    "7. Visualizations of Simulation using Plotly.\n",
    "8. Biases and Future Improvements for the project. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information & Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Explanations and Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Imports here:\n",
    "import DataFunctions\n",
    "import cfbd\n",
    "from cfbd.rest import ApiException\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import networx as nx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, we need College Football Data and Locations Data for the simulation, which will be obtained using CFBD and BingMaps API's. They typically require registering and getting an unique key, this is done to keep track of who and how many API calls are needed. Some API's may ask you to pay after certain number of calls, but for this project is not needed. <br>\n",
    "\n",
    "College football data is needed for all the simulation purposes, extracting all FBS teams and their season stats. First install using command: `\"pip install cfbd\"`. Then register on https://collegefootballdata.com/key to obtain a key and creating a configuration object with that data, we will be able to use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cfbd\n",
    "# from cfbd.rest import ApiException\n",
    "\n",
    "configuration = cfbd.Configuration()\n",
    "# configuration.api_key['Authorization'] = 'YOUR_KEY_HERE'\n",
    "configuration.api_key['Authorization'] = '3WCU5V2X05Rvh60ZxUG8FarJN4s2D1lcd2c2r6Kz/qL1Y3tVBJtWsuNATnzHRV2h'\n",
    "configuration.api_key_prefix['Authorization'] = 'Bearer'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to extract data from 3 classes: `GamesApi` which gives us data for all the games; `BettingApi` which gives us data for all the betting lines; and `StatsApi` which gives us the team season data.<br>\n",
    "\n",
    "In our predictive modelling, we are going to use a merge between game-data and team-data as our predictors, and betting lines data as our target. Down below is an example for data extraction, more detail was done on functions inside `DataFunctions.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = cfbd.GamesApi(cfbd.ApiClient(configuration))\n",
    "year=2022\n",
    "division=\"fbs\"\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_games(year=year,division=division)\n",
    "    print(len(api_response))\n",
    "except:\n",
    "    print(ApiException)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explaining how functions are setup on DataFunctions.py from API to DF to SQL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before inserting code into our SQL database, we have to create instances for each sub-API. Also is needed a list of conferences that can be obtained directly from the distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "\n",
    "# Creating API Instances:\n",
    "api_instance = cfbd.GamesApi(cfbd.ApiClient(configuration))\n",
    "api_instance1 = cfbd.BettingApi(cfbd.ApiClient(configuration))\n",
    "api_instance2 = cfbd.StatsApi(cfbd.ApiClient(configuration))\n",
    "\n",
    "#creating cols to work on selected stats data\n",
    "cols = ['team','season','conference','Offensive_ppa','Offensive_success_rate',\n",
    "        'Offensive_explosiveness','Offensive_power_success',\n",
    "        'Offensive_stuff_rate','Offensive_line_yards',\n",
    "        'Defensive_ppa','Defensive_success_rate',\n",
    "        'Defensive_explosiveness','Defensive_power_success',\n",
    "        'Defensive_stuff_rate','Defensive_line_yards',\n",
    "        'Offensive_havoc_total','Offensive_rushing_plays_ppa',\n",
    "        'Offensive_rushing_plays_success_rate',\n",
    "        'Offensive_rushing_plays_explosiveness',\n",
    "        'Offensive_passing_plays_ppa',\n",
    "        'Offensive_passing_plays_success_rate',\n",
    "        'Offensive_passing_plays_explosiveness',\n",
    "        'Defensive_havoc_total','Defensive_rushing_plays_ppa',\n",
    "        'Defensive_rushing_plays_success_rate',\n",
    "        'Defensive_rushing_plays_explosiveness',\n",
    "        'Defensive_passing_plays_ppa',\n",
    "        'Defensive_passing_plays_success_rate',\n",
    "        'Defensive_passing_plays_explosiveness']\n",
    "\n",
    "for year in range(2015,2023):\n",
    "    #obtaining games data\n",
    "    gamelist = DataFunctions.get_fbs_games(api_instance=api_instance,year=year)\n",
    "    games_df = DataFunctions.df_from_games(gamelist=gamelist)\n",
    "    \n",
    "    #creating conferences to obtain data from them:\n",
    "    conferences=[]\n",
    "    for game in gamelist:\n",
    "        conferences.append(game.away_conference)\n",
    "    conferences=set(conferences)\n",
    "    \n",
    "    #obtaining betting data\n",
    "    betting_list=DataFunctions.get_fbs_betting(api_instance=api_instance1,year=year,conferences=conferences)\n",
    "    betting_df=DataFunctions.df_betting_lines(betting_list)\n",
    "    \n",
    "    #obtaining stats data\n",
    "    teamstats = api_instance2.get_advanced_team_season_stats(year=year)\n",
    "    stats_df = DataFunctions.df_team_advstats(teamstats=teamstats)\n",
    "    stats_df = DataFunctions.df_stats_needed(stats_df,cols)\n",
    "    \n",
    "    #inserting all dataframes into sql databases.\n",
    "    conn = sqlite3.connect(\"CollegeFootball.db\")\n",
    "    games_df.to_sql(\"games\",conn,if_exists=\"append\",index=False)\n",
    "    betting_df.to_sql(\"betting_lines\",conn,if_exists=\"append\",index=False)\n",
    "    stats_df.to_sql(\"stats\",conn,if_exists=\"append\",index=False)\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't need more API calls for College Football as all relevants teams on our simulation and their games are included. Modifications can be made to consider FBS teams or other years.<br>\n",
    "Now that we have all the information we need for our predictive model in the SQL database, we need to preprocess it to be able to create models with it. <br>\n",
    "The main idea is to have our predictors be a combination of games and teams data, therefore we need to create a dataframe which correctly retrieves data considering game results, home and away team data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the code below helps the SQL query to obtain the proper columns and sets up their proper labels. As we are obtaining 2 rows of data of the stats table, we need to properly rename the columns to differentiate Home and Away statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcols = games_df.columns\n",
    "gstr = \"\"\n",
    "for c in gcols:\n",
    "    gstr += \"G.\"+str(c)+\",\"\n",
    "gstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcols = betting_df.columns\n",
    "bstr = \"\"\n",
    "for b in bcols:\n",
    "    bstr += \"B.\"+str(b)+\",\"\n",
    "bstr = bstr[5:]\n",
    "bstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"\"\n",
    "for c in cols:\n",
    "   s1 += \"S1.\" + str(c) +  \" AS Home_\" + str(c) + \", \"\n",
    "s1 = s1[:-1]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = \"\"\n",
    "for c in cols:\n",
    "   s2 += \"S2.\" + str(c) +  \" AS Away_\" + str(c) + \", \"\n",
    "s2 = s2[:-2]\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd=\\\n",
    "f\"\"\"\n",
    "SELECT {str(gstr)} {str(bstr)} {str(s1)} {str(s2)}\n",
    "FROM games G\n",
    "INNER JOIN betting_lines B ON G.id=B.id\n",
    "INNER JOIN stats S1 ON S1.team=G.home_team\n",
    "INNER JOIN stats S2 ON S2.team=G.away_team\n",
    "WHERE (S2.season=G.season AND S1.season=G.season)\n",
    "\"\"\"\n",
    "\n",
    "conn=sqlite3.connect(\"CollegeFootball.db\")\n",
    "df_merged=pd.read_sql_query(cmd,conn)\n",
    "conn.close() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the merge we have the following merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to drop some columns to create our predictors dataframe and our target dataframe using betting lines as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors df\n",
    "parameters_df=df_merged.drop(['id','season', 'home_id', 'home_team',\n",
    "       'home_conference', 'home_points', 'away_id', 'away_team',\n",
    "       'away_conference', 'away_points', 'game_spread', 'game_totalpts',\n",
    "       'av_spread', 'av_total'], axis=1)\n",
    "parameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target df\n",
    "predict_betting_df=df_merged[['av_spread','av_total','id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(DISTANCES API)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model using Tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrays to use on tensorflow and create our first predictive model, which uses betting lines as predictors. Using a train_test split of 70% train and 30% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame()\n",
    "X = np.array(parameters_df,dtype=np.float32)\n",
    "y_betting = np.array(predict_betting_df)    #predicting betting info\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_betting,test_size=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a simple 2 layer neural network using tensorflow. We use the Sequential model, which simply allows us to create a model layer by layer. We compile the model using 'adam' optimizer, an efficient variation of Gradient Descent, and using 'mse' Mean Squared Error as the loss function to minimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.Dense(100,input_shape=(X_train.shape[1],),activation='relu'),\n",
    "    layers.Dense(100,activation='relu'),\n",
    "    layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mae','mse'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to train our model for 100 epochs, which should be enough to fit the model. After that we can display the performance of the model by plotting the error function progress and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train[:,:2],epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "plt.plot(history.history[\"mse\"][10:])\n",
    "plt.gca().set(xlabel=\"epoch\",ylabel=\"mse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating on test data\n",
    "model.evaluate(X_test,y_test[:,:2],verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spread distribution\n",
    "plt.hist(predictions[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total points distribution\n",
    "plt.hist(predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating boxplot\n",
    "diff = predictions-y_betting[:,:2]\n",
    "diffmean = diff.mean(axis=0)\n",
    "bestdiffstd = diff.std(axis=0)   #renamed variable so it's unique to this model\n",
    "fig, ax = plt.subplots()\n",
    "bp = ax.boxplot(diff,showmeans=True)\n",
    "for i, line in enumerate(bp['medians']):\n",
    "    x, y = line.get_xydata()[1]\n",
    "    text = ' μ={:.2f}\\n σ={:.2f}'.format(diffmean[i], bestdiffstd[i])\n",
    "    ax.annotate(text, xy=(x, y))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Working with Home and Away Pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Working with Actual Spread and Pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining Findings and decision on working only on Betting Lines. Run example of a single game prediction. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairing System using Minimum Weight Matching Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using teams locations extracted before, we run the Pairing Algorithm using Minimum Weight Matching and Networkx. Which does the following: Selects pairs of vertices where the sum of those edges is minimized. First we retrieve data from the database, then we use NetworkX package to create the Graph and do the pairings, which afterwards we Simulate games using our Neural Network and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"CollegeFootball.db\")\n",
    "distances = pd.read_sql_query(\"SELECT * FROM distances\",conn)\n",
    "conn.close()\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "m_dist = np.round(np.array(distances),decimals=3)\n",
    "L = []\n",
    "for k in range(126):\n",
    "    for j in range(126):\n",
    "        if k>j: L.append((k,j,m_dist[k,j]))\n",
    "        \n",
    "CollegeGraph = nx.Graph()\n",
    "CollegeGraph.add_weighted_edges_from(L)\n",
    "curr_data = np.zeros(shape=(126,3),dtype=int)\n",
    "curr_data[:,0] = np.arange(126)\n",
    "\n",
    "for i in range(12):\n",
    "    DataFunctions.Simulate(g=CollegeGraph,\n",
    "                           i=i,c=curr_data,\n",
    "                           y=2022,st_dev=bestdiffstd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The code below allows us to remove the previously simulated games from our database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect(\"CollegeFootball.db\")\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(\"DROP TABLE simul_games\")\n",
    "# conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Insert Visualizations Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Visualizations of Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Insert Plotly visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
